# PROP-0015: Capture Web Content Tool

**Status:** draft
**Priority:** high
**Created:** 2026-01-06
**Author:** jordan + housekeeping
**Project:** agency

## Problem

Modern web platforms (Threads, Twitter/X, Reddit, LinkedIn, Perplexity) are:
- JavaScript-heavy (content not in initial HTML)
- Authentication-gated (require login to view)
- Scraper-hostile (block bots, rate limit)
- Dynamic (content loads via API calls after page load)

Agents cannot capture valuable content from these platforms using standard web fetch. This content often contains insights, discussions, research results, and references valuable for knowledge bases.

**Specific pain points:**
- **Threads/Twitter** - Social posts, thought leadership
- **Reddit** - Discussions, community knowledge
- **Perplexity** - Research results, citations, summaries
- **LinkedIn** - Professional insights, announcements

## Origin

While trying to capture a Threads post from Boris Cherny (creator of Claude Code) about his 100% AI-generated production workflow, standard WebFetch failed - the page was too JS-heavy and returned no useful content.

Jordan: "You can drive Chrome, can't you? Maybe we come up with a way that you drive Chrome and get the content? Bypasses AUTH and things like that?"

## Proposal

Create `./tools/capture-web` that connects to the principal's **running Chrome browser** via Chrome DevTools Protocol (CDP). This leverages existing authentication sessions - no credential storage needed.

### Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    PRINCIPAL'S CHROME                            │
│                                                                  │
│  Already logged into:                                            │
│  • Threads  • Twitter  • Reddit  • LinkedIn  • GitHub           │
│  • Notion   • Slack    • Google  • etc.                         │
│                                                                  │
│  Chrome started with: --remote-debugging-port=9222              │
└─────────────────────────────────────────────────────────────────┘
                              │
                              │ CDP (Chrome DevTools Protocol)
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    ./tools/capture-web                           │
│                                                                  │
│  1. Connect to Chrome via CDP                                    │
│  2. Navigate to URL (or use current tab)                         │
│  3. Wait for JS to render                                        │
│  4. Extract clean text content                                   │
│  5. Optionally take screenshot                                   │
│  6. Save to knowledge base with metadata                         │
└─────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│              claude/knowledge/captured/                          │
│                                                                  │
│  2026-01-06-threads-boris-cherny-commits.md                     │
│  2026-01-06-twitter-anthropic-announcement.md                   │
│  2026-01-06-reddit-claude-code-discussion.md                    │
└─────────────────────────────────────────────────────────────────┘
```

### Usage

```bash
# Capture current page in Chrome
./tools/capture-web

# Capture specific URL
./tools/capture-web "https://threads.net/@boris_cherny/post/xyz"

# Capture with title for knowledge base
./tools/capture-web --title "Boris Cherny on AI Commits" "https://..."

# Capture with screenshot
./tools/capture-web --screenshot "https://..."

# Just extract text (don't save to KB)
./tools/capture-web --stdout "https://..."
```

### Setup (One-Time)

Principal needs to start Chrome with remote debugging enabled:

**macOS:**
```bash
/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222
```

**Or add to shell profile:**
```bash
alias chrome-debug='/Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222'
```

**Or use the tool to set it up:**
```bash
./tools/setup-chrome-capture
```

### Output Format

Captured content saved as markdown:

```markdown
# Boris Cherny on 100% AI-Generated Commits

**Source:** https://threads.net/@boris_cherny/post/xyz
**Captured:** 2026-01-06 12:34 SGT
**Platform:** Threads
**Author:** @boris_cherny

---

In the last 30 days, I landed 259 PRs and 497 commits across my projects.
Around 40k lines added and 38k removed.

Every single line was generated by Claude Code running on Opus 4.5.

[Full content here...]

---

*Captured by ./tools/capture-web*
```

### Implementation

**Dependencies:**
- `chrome-remote-interface` (npm) - CDP client
- Or native WebSocket to CDP endpoint

**Core Logic:**

```javascript
const CDP = require('chrome-remote-interface');

async function captureWeb(url) {
  const client = await CDP({ port: 9222 });
  const { Page, Runtime, DOM } = client;

  await Page.enable();
  await Page.navigate({ url });
  await Page.loadEventFired();

  // Wait for JS to render
  await new Promise(r => setTimeout(r, 2000));

  // Extract text content
  const { result } = await Runtime.evaluate({
    expression: `
      // Remove scripts, styles, nav, footer
      document.querySelectorAll('script, style, nav, footer, header')
        .forEach(el => el.remove());

      // Get main content
      const main = document.querySelector('main, article, [role="main"]')
                   || document.body;

      main.innerText;
    `
  });

  await client.close();
  return result.value;
}
```

### Platform-Specific Extractors

Different platforms need different selectors:

| Platform | Content Selector |
|----------|------------------|
| Threads | `[data-pressable-container] > div` |
| Twitter/X | `article[data-testid="tweet"]` |
| Reddit | `.Post` or `shreddit-post` |
| LinkedIn | `.feed-shared-update-v2__description` |
| Perplexity | `.prose` or answer container |
| Generic | `main, article, [role="main"]` |

### Security Considerations

- **No credentials stored** - Uses existing browser sessions
- **Principal controls Chrome** - Must explicitly start with debug port
- **Local only** - CDP connection is localhost
- **Read-only** - Tool only reads, doesn't post/modify

### Integration with Knowledge Base

Captured content can be:
1. Saved directly to `claude/knowledge/captured/`
2. Indexed by Knowledge Indexer (PROP-0014)
3. Referenced by TheCaptain

### Free Tier

This goes in the **base offering** because:
- Solves a fundamental problem (capturing web content)
- Uses principal's own browser (no infrastructure cost)
- Enables knowledge accumulation for all users

## Open Questions

- [ ] Handle multi-page threads (Twitter threads, Reddit comments)?
- [ ] Screenshot vs text - when to capture both?
- [ ] Rate limiting to avoid overwhelming Chrome?
- [ ] Support Firefox via Remote Debugging Protocol?

## Dependencies

- Chrome with remote debugging
- Node.js (for CDP client)
- Related: PROP-0014 (Knowledge Indexer)

## When Approved

- Becomes: INSTR-XXXX
- Assigned to: housekeeping
- Target: v0.1.0 (core tool)

---

## Discussion Log

### 2026-01-06 - Origin

While trying to capture Boris Cherny's Threads post about 100% AI-generated commits, WebFetch failed due to JS-heavy pages.

Jordan: "You can drive Chrome, can't you? Maybe we come up with a way that you drive Chrome and get the content? Bypasses AUTH and things like that?"

Decision: Use Chrome DevTools Protocol to connect to principal's running Chrome, leveraging existing auth sessions.

Jordan: "I think [CDP] makes the most sense. And this is a tool that we can put in the base offering."
