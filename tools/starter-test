#!/bin/bash
# starter-test - Complete test suite for the-agency-starter
#
# Usage:
#   ./tools/starter-test              # Run all tests
#   ./tools/starter-test --local      # Use local starter (skip GitHub fetch)
#   ./tools/starter-test --keep       # Don't cleanup after tests
#   ./tools/starter-test --verbose    # Show detailed output
#
# Tests:
#   1. Install via install.sh
#   2. Create new project via project-create
#   3. Compare installed vs source
#   4. Compare new project vs expected structure
#   5. Verify no sensitive content
#   6. Test basic tools work
#   7. Batch update error handling
#   9. Pre-update verification checks
#  10. Edge cases (non-git, dirty, corrupt)

# Don't use set -e as we want to continue on test failures
# set -e

TOOL_VERSION="1.1.0-20260120-000001"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
DIM='\033[2m'
NC='\033[0m'

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Source log helper for telemetry
SCRIPT_DIR="${SCRIPT_DIR:-$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)}"
if [[ -f "$SCRIPT_DIR/_log-helper" ]]; then
    source "$SCRIPT_DIR/_log-helper"
fi
RUN_ID=$(log_start "starter-test" "agency-tool" "$@" 2>/dev/null) || true
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"
TEST_DIR="$(dirname "$PROJECT_ROOT")/test/starter-test-run"
STARTER_SOURCE="$(cd "$PROJECT_ROOT/../the-agency-starter" 2>/dev/null && pwd)"

# Options
USE_LOCAL=false
KEEP_ARTIFACTS=false
VERBOSE=false

# Counters
TESTS_RUN=0
TESTS_PASSED=0
TESTS_FAILED=0

# Parse arguments
for arg in "$@"; do
    case $arg in
        --version|-v)
            echo "starter-test $TOOL_VERSION"
            exit 0
            ;;
        --help|-h)
            echo "starter-test - Complete test suite for the-agency-starter"
            echo ""
            echo "Usage:"
            echo "  ./tools/starter-test              # Run all tests"
            echo "  ./tools/starter-test --local      # Use local starter (skip GitHub)"
            echo "  ./tools/starter-test --keep       # Don't cleanup after tests"
            echo "  ./tools/starter-test --verbose    # Show detailed output"
            echo ""
            echo "Tests performed:"
            echo "  1. Install via install.sh"
            echo "  2. Create new project via project-create"
            echo "  3. Compare installed vs source"
            echo "  4. Compare new project vs expected"
            echo "  5. Verify no sensitive content"
            echo "  6. Test basic tools"
            echo "  7. Batch update error handling"
            echo "  8. Pre-update verification checks"
            echo "  9. Edge cases (non-git, dirty, corrupt)"
            echo " 10. project-update --check --json tests"
            echo " 11. project-create manifest and registry tests"
            exit 0
            ;;
        --local|-l)
            USE_LOCAL=true
            ;;
        --keep|-k)
            KEEP_ARTIFACTS=true
            ;;
        --verbose|-V)
            VERBOSE=true
            ;;
    esac
done

# Logging
log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }
log_step() { echo -e "${BLUE}[TEST]${NC} $1"; }
log_detail() { [[ "$VERBOSE" == "true" ]] && echo -e "${DIM}       $1${NC}"; }
log_pass() { echo -e "  ${GREEN}✓${NC} $1"; ((TESTS_PASSED++)); ((TESTS_RUN++)); }
log_fail() { echo -e "  ${RED}✗${NC} $1"; ((TESTS_FAILED++)); ((TESTS_RUN++)); }

# Cleanup function
cleanup() {
    if [[ "$KEEP_ARTIFACTS" == "false" && -d "$TEST_DIR" ]]; then
        log_info "Cleaning up test artifacts..."
        rm -rf "$TEST_DIR"
    fi
}

# Setup test directory
setup() {
    log_info "Setting up test environment..."

    # Check starter source exists
    if [[ -z "$STARTER_SOURCE" || ! -d "$STARTER_SOURCE" ]]; then
        log_error "Starter source not found at expected location"
        echo "Expected: $PROJECT_ROOT/../the-agency-starter"
        exit 1
    fi
    log_detail "Starter source: $STARTER_SOURCE"

    # Clean and create test directory
    rm -rf "$TEST_DIR"
    mkdir -p "$TEST_DIR"
    log_detail "Test directory: $TEST_DIR"
}

# Test 1: Install via install.sh
test_install() {
    log_step "Test 1: Install via install.sh"

    local install_dir="$TEST_DIR/installed-starter"

    if [[ "$USE_LOCAL" == "true" ]]; then
        # Copy directly from local source (bypass install.sh which clones from GitHub)
        log_detail "Using local source directly"
        mkdir -p "$install_dir"
        rsync -a \
            --exclude='.git' \
            --exclude='node_modules' \
            --exclude='.next' \
            --exclude='target' \
            --exclude='dist' \
            --exclude='.DS_Store' \
            "$STARTER_SOURCE/" "$install_dir/"
        chmod +x "$install_dir/tools/"* 2>/dev/null || true
    else
        # Try to fetch from GitHub
        log_detail "Trying GitHub install..."
        cd "$TEST_DIR"
        if curl -fsSL https://raw.githubusercontent.com/the-agency-ai/the-agency-starter/main/install.sh 2>/dev/null | AGENCY_INSTALL_DIR="$install_dir" bash > /dev/null 2>&1; then
            log_detail "GitHub install succeeded"
        else
            log_warn "GitHub not accessible, falling back to local"
            AGENCY_INSTALL_DIR="$install_dir" bash "$STARTER_SOURCE/install.sh" > /dev/null 2>&1
        fi
    fi

    # Verify installation
    if [[ -d "$install_dir" ]]; then
        log_pass "install.sh created directory"
    else
        log_fail "install.sh did not create directory"
        return 1
    fi

    if [[ -f "$install_dir/CLAUDE.md" ]]; then
        log_pass "CLAUDE.md exists"
    else
        log_fail "CLAUDE.md missing"
    fi

    if [[ -d "$install_dir/tools" ]]; then
        log_pass "tools/ directory exists"
    else
        log_fail "tools/ directory missing"
    fi

    if [[ -d "$install_dir/source" ]]; then
        log_pass "source/ directory exists"
    else
        log_fail "source/ directory missing"
    fi

    if [[ -x "$install_dir/tools/myclaude" ]]; then
        log_pass "tools are executable"
    else
        log_fail "tools not executable"
    fi
}

# Test 2: Create new project
test_new_project() {
    log_step "Test 2: Create new project via project-create"

    local starter_dir="$TEST_DIR/installed-starter"
    local project_dir="$TEST_DIR/test-project"

    # Run project-create (suppress the claude launch at the end)
    cd "$starter_dir"
    ./tools/project-create "$project_dir" --no-launch > /dev/null 2>&1 || true

    # Verify project creation
    if [[ -d "$project_dir" ]]; then
        log_pass "project-create created directory"
    else
        log_fail "project-create did not create directory"
        return 1
    fi

    if [[ -d "$project_dir/.git" ]]; then
        log_pass "git repository initialized"
    else
        log_fail "git repository not initialized"
    fi

    if [[ -f "$project_dir/CLAUDE.md" ]]; then
        log_pass "CLAUDE.md exists in project"
    else
        log_fail "CLAUDE.md missing from project"
    fi

    if [[ -d "$project_dir/claude/agents/captain" ]]; then
        log_pass "captain agent exists"
    else
        log_fail "captain agent missing"
    fi

    if [[ -d "$project_dir/source/apps" ]]; then
        log_pass "source/apps exists"
    else
        log_fail "source/apps missing"
    fi
}

# Test 3: Compare installed vs source
test_compare_install() {
    log_step "Test 3: Compare installed starter vs source"

    local installed="$TEST_DIR/installed-starter"
    local source="$STARTER_SOURCE"

    # Key files that must match
    local key_files=(
        "CLAUDE.md"
        "README.md"
        "VERSION"
        "claude/agents/captain/agent.md"
        "claude/config/agency.yaml"
    )

    for file in "${key_files[@]}"; do
        if [[ -f "$installed/$file" && -f "$source/$file" ]]; then
            if diff -q "$installed/$file" "$source/$file" > /dev/null 2>&1; then
                log_pass "$file matches source"
            else
                log_fail "$file differs from source"
            fi
        else
            log_fail "$file missing in installed or source"
        fi
    done

    # Count files
    local source_count=$(find "$source" -type f -not -path '*/.git/*' | wc -l | tr -d ' ')
    local installed_count=$(find "$installed" -type f -not -path '*/.git/*' | wc -l | tr -d ' ')

    log_detail "Source files: $source_count, Installed files: $installed_count"

    if [[ "$source_count" -eq "$installed_count" ]]; then
        log_pass "File count matches ($source_count files)"
    else
        log_warn "File count differs (source: $source_count, installed: $installed_count)"
    fi
}

# Test 4: Compare new project vs expected
test_compare_project() {
    log_step "Test 4: Compare new project structure"

    local project="$TEST_DIR/test-project"

    # Required directories
    local required_dirs=(
        "claude"
        "claude/agents"
        "claude/agents/captain"
        "claude/config"
        "claude/docs"
        "claude/principals"
        "claude/workstreams"
        "source"
        "source/apps"
        "source/services"
        "tools"
    )

    for dir in "${required_dirs[@]}"; do
        if [[ -d "$project/$dir" ]]; then
            log_pass "$dir/ exists"
        else
            log_fail "$dir/ missing"
        fi
    done

    # Required files
    local required_files=(
        "CLAUDE.md"
        "README.md"
        "VERSION"
        ".gitignore"
        "tools/myclaude"
        "tools/agent-create"
        "tools/workstream-create"
    )

    for file in "${required_files[@]}"; do
        if [[ -f "$project/$file" ]]; then
            log_pass "$file exists"
        else
            log_fail "$file missing"
        fi
    done
}

# Test 5: Verify no sensitive content
test_sensitive_content() {
    log_step "Test 5: Verify no sensitive content"

    local installed="$TEST_DIR/installed-starter"
    local project="$TEST_DIR/test-project"

    # API key patterns that should NOT exist in any file
    local api_patterns=(
        "sk-ant-"
        "ghp_"
    )

    # Check for API key patterns
    for pattern in "${api_patterns[@]}"; do
        if grep -r "$pattern" "$installed" --include="*.md" --include="*.yaml" --include="*.json" --include="*.env" 2>/dev/null | grep -v ".git" > /dev/null; then
            log_fail "Found '$pattern' in installed starter"
        else
            log_pass "No '$pattern' in installed starter"
        fi
    done

    # Actual secret FILES that should NOT exist
    local secret_files=(
        "discord.env"
        "gumroad.env"
        "github.env"
    )

    # Check for actual secret files
    for file in "${secret_files[@]}"; do
        if find "$installed" -name "$file" -type f 2>/dev/null | grep -q .; then
            log_fail "Found actual file '$file' in installed starter"
        else
            log_pass "No actual '$file' file in installed starter"
        fi
    done

    # Check for jordan directory (should not exist)
    if [[ -d "$installed/claude/principals/jordan" ]]; then
        log_fail "jordan/ directory exists in installed"
    else
        log_pass "No jordan/ directory in installed"
    fi

    # Check for SESSION files (should not exist)
    if find "$installed" -name "SESSION-*.md" 2>/dev/null | grep -q .; then
        log_fail "SESSION files found in installed"
    else
        log_pass "No SESSION files in installed"
    fi

    # Check config doesn't have jdm: jordan
    if grep -q "jdm: jordan" "$installed/claude/config/agency.yaml" 2>/dev/null; then
        log_fail "jdm: jordan found in config"
    else
        log_pass "No jdm: jordan in config"
    fi
}

# Test 6: Test basic tools
test_tools() {
    log_step "Test 6: Test basic tools work"

    local project="$TEST_DIR/test-project"
    cd "$project"

    # Test ./tools/now
    if ./tools/now > /dev/null 2>&1; then
        log_pass "./tools/now works"
    else
        log_fail "./tools/now failed"
    fi

    # Test ./tools/agentname (outputs agent name or 'unknown')
    local agentname_output=$(./tools/agentname 2>&1)
    if [[ -n "$agentname_output" ]]; then
        log_pass "./tools/agentname works (output: $agentname_output)"
    else
        log_fail "./tools/agentname produced no output"
    fi

    # Test ./tools/workstream
    if ./tools/workstream 2>&1 | grep -q "WORKSTREAM not set\|housekeeping\|unknown"; then
        log_pass "./tools/workstream works"
    else
        log_fail "./tools/workstream failed unexpectedly"
    fi

    # Test ./tools/tool-find
    if ./tools/tool-find -l > /dev/null 2>&1; then
        log_pass "./tools/tool-find works"
    else
        log_fail "./tools/tool-find failed"
    fi
}

# Test 7: Batch Update Error Handling
test_batch_update_errors() {
    log_step "Test 7: Batch update error handling"

    local starter="$TEST_DIR/installed-starter"
    local test_projects_dir="$TEST_DIR/batch-test"
    mkdir -p "$test_projects_dir"

    # Test 1: Missing projects.json
    log_detail "Testing missing projects.json..."
    # Backup existing projects.json
    local projects_backup=""
    if [[ -f "$starter/.agency/projects.json" ]]; then
        projects_backup=$(cat "$starter/.agency/projects.json")
    fi
    rm -f "$starter/.agency/projects.json"

    # The batch scripts should handle missing file gracefully
    local result=$(cat "$starter/.agency/projects.json" 2>&1 | python3 -c "
import json, sys
try:
    data = json.load(sys.stdin)
    print('loaded')
except:
    print('error')
" 2>/dev/null || echo "file_missing")

    if [[ "$result" == "file_missing" || "$result" == "error" ]]; then
        log_pass "Missing projects.json handled (no crash)"
    else
        log_fail "Unexpected behavior with missing projects.json"
    fi

    # Test 2: Empty projects.json
    log_detail "Testing empty projects.json..."
    mkdir -p "$starter/.agency"
    echo '{"schema_version": "1.0", "projects": []}' > "$starter/.agency/projects.json"

    local count=$(cat "$starter/.agency/projects.json" | python3 -c "
import json, sys
data = json.load(sys.stdin)
print(len(data.get('projects', [])))
" 2>/dev/null || echo "error")

    if [[ "$count" == "0" ]]; then
        log_pass "Empty projects.json handled correctly"
    else
        log_fail "Empty projects.json not handled correctly"
    fi

    # Test 3: Non-existent project path
    log_detail "Testing non-existent project path..."
    echo '{"schema_version": "1.0", "projects": [{"name": "fake", "path": "/nonexistent/path", "created_at": "2026-01-01T00:00:00Z", "starter_version": "1.0.0"}]}' > "$starter/.agency/projects.json"

    local path_check=$(cat "$starter/.agency/projects.json" | python3 -c "
import json, sys, os
data = json.load(sys.stdin)
for p in data.get('projects', []):
    if not os.path.exists(p['path']):
        print('not_found')
        break
" 2>/dev/null || echo "error")

    if [[ "$path_check" == "not_found" ]]; then
        log_pass "Non-existent path detected correctly"
    else
        log_fail "Non-existent path not detected"
    fi

    # Cleanup - restore original projects.json
    if [[ -n "$projects_backup" ]]; then
        mkdir -p "$starter/.agency"
        echo "$projects_backup" > "$starter/.agency/projects.json"
    fi
}

# Test 8: Pre-Update Verification
test_pre_update_verification() {
    log_step "Test 8: Pre-update verification checks"

    local project="$TEST_DIR/test-project"
    cd "$project"

    # Test 1: Git status check works
    log_detail "Testing git status check..."
    local git_status=$(git status --short 2>/dev/null)
    if [[ -z "$git_status" ]]; then
        log_pass "Git status clean check works"
    else
        log_pass "Git status dirty check works (found changes)"
    fi

    # Test 2: Manifest modification check
    log_detail "Testing manifest modification check..."
    if [[ -f ".agency/manifest.json" ]]; then
        local mod_check=$(cat .agency/manifest.json | python3 -c "
import json, sys
manifest = json.load(sys.stdin)
modified = [f for f, info in manifest.get('files', {}).items() if info.get('modified')]
print(len(modified))
" 2>/dev/null || echo "error")

        if [[ "$mod_check" =~ ^[0-9]+$ ]]; then
            log_pass "Manifest modification check works ($mod_check modified files)"
        else
            log_fail "Manifest modification check failed"
        fi
    else
        log_pass "No manifest yet (expected for new project)"
    fi

    # Test 3: Project-update --check works (if manifest exists)
    log_detail "Testing project-update --check..."
    local starter="$TEST_DIR/installed-starter"

    # First init the manifest
    "$starter/tools/project-update" --init --from="$starter" > /dev/null 2>&1 || true

    if [[ -f ".agency/manifest.json" ]]; then
        if "$starter/tools/project-update" --check --from="$starter" > /dev/null 2>&1; then
            log_pass "project-update --check works"
        else
            log_fail "project-update --check failed"
        fi
    else
        log_pass "Skipping --check test (no manifest)"
    fi
}

# Test 9: Edge Cases
test_edge_cases() {
    log_step "Test 9: Edge cases"

    local starter="$TEST_DIR/installed-starter"
    local edge_dir="$TEST_DIR/edge-cases"
    mkdir -p "$edge_dir"

    # Test 1: Project path exists but is not a git repo
    log_detail "Testing non-git directory..."
    local non_git_dir="$edge_dir/not-a-repo"
    mkdir -p "$non_git_dir"
    touch "$non_git_dir/some-file.txt"

    if git -C "$non_git_dir" status > /dev/null 2>&1; then
        log_fail "Non-git directory incorrectly detected as git repo"
    else
        log_pass "Non-git directory correctly detected"
    fi

    # Test 2: Project has uncommitted changes (dirty check)
    log_detail "Testing dirty repo detection..."
    local dirty_dir="$edge_dir/dirty-repo"
    mkdir -p "$dirty_dir"
    cd "$dirty_dir"
    git init --quiet
    touch file.txt
    git add file.txt
    git commit -m "init" --quiet

    # Make it dirty
    echo "change" >> file.txt

    local dirty_status=$(git status --short 2>/dev/null)
    if [[ -n "$dirty_status" ]]; then
        log_pass "Dirty repo correctly detected"
    else
        log_fail "Dirty repo not detected"
    fi

    # Test 3: Corrupt manifest.json
    log_detail "Testing corrupt manifest handling..."
    local corrupt_dir="$edge_dir/corrupt-manifest"
    mkdir -p "$corrupt_dir/.agency"
    echo "not valid json{{{" > "$corrupt_dir/.agency/manifest.json"

    local corrupt_check=$(cat "$corrupt_dir/.agency/manifest.json" | python3 -c "
import json, sys
try:
    json.load(sys.stdin)
    print('valid')
except:
    print('invalid')
" 2>/dev/null || echo "error")

    if [[ "$corrupt_check" == "invalid" ]]; then
        log_pass "Corrupt manifest correctly detected"
    else
        log_fail "Corrupt manifest not detected"
    fi

    # Cleanup
    cd "$PROJECT_ROOT"
}

# Test 10: project-update --check --json tests
test_update_check_json() {
    log_step "Test 10: project-update --check --json tests"

    local project="$TEST_DIR/test-project"
    local starter="$TEST_DIR/installed-starter"
    cd "$project"

    # Ensure manifest exists
    if [[ ! -f ".agency/manifest.json" ]]; then
        ./tools/project-update --init --from="$starter" > /dev/null 2>&1 || true
    fi

    # Test 10a: --check mode returns without error
    local check_output
    if check_output=$(./tools/project-update --check --from="$starter" 2>&1); then
        log_pass "--check mode runs successfully"
    else
        log_fail "--check mode failed: $check_output"
    fi

    # Test 10b: --check --json outputs valid JSON
    local json_output=$(./tools/project-update --check --json --from="$starter" 2>/dev/null)
    if echo "$json_output" | python3 -c "import json,sys; json.load(sys.stdin)" 2>/dev/null; then
        log_pass "--check --json outputs valid JSON"
    else
        log_fail "--check --json output is not valid JSON"
    fi

    # Test 10c: JSON contains all required fields
    if echo "$json_output" | python3 -c "
import json, sys
d = json.load(sys.stdin)
required = ['current_version', 'latest_version', 'updates_available', 'files_to_update', 'files_modified_locally', 'components_outdated', 'breaking_changes']
assert all(k in d for k in required), f'Missing fields: {[k for k in required if k not in d]}'
" 2>/dev/null; then
        log_pass "JSON contains all required fields"
    else
        log_fail "JSON missing required fields"
    fi

    # Test 10d: Test with locally modified file
    echo "# Test modification for check" >> CLAUDE.md
    local modified_output=$(./tools/project-update --check --json --from="$starter" 2>/dev/null)
    local modified_detected=$(echo "$modified_output" | python3 -c "
import json, sys
d = json.load(sys.stdin)
modified = d.get('files_modified_locally', [])
updates = d.get('files_to_update', [])
# Either files_modified_locally has entries, or CLAUDE.md is in files_to_update
if modified or 'CLAUDE.md' in updates:
    print('detected')
else:
    print('not_detected')
" 2>/dev/null || echo "error")

    if [[ "$modified_detected" == "detected" ]]; then
        log_pass "Locally modified files detected"
    elif [[ "$modified_detected" == "not_detected" ]]; then
        log_warn "Modification not detected (file may not be tracked yet)"
    else
        log_fail "Modification detection failed: $modified_detected"
    fi

    # Restore file
    git checkout CLAUDE.md 2>/dev/null || true

    # Test 10e: updates_available is boolean
    local bool_check=$(echo "$json_output" | python3 -c "
import json, sys
d = json.load(sys.stdin)
if isinstance(d.get('updates_available'), bool):
    print('valid')
else:
    print('invalid')
" 2>/dev/null || echo "error")

    if [[ "$bool_check" == "valid" ]]; then
        log_pass "updates_available is boolean type"
    else
        log_fail "updates_available is not boolean type"
    fi
}

# Test 11: project-create manifest and registry tests
test_new_manifest_registry() {
    log_step "Test 11: project-create manifest and registry tests"

    local project="$TEST_DIR/test-project"
    local starter="$TEST_DIR/installed-starter"

    # Test 11a: manifest.json exists in project
    if [[ -f "$project/.agency/manifest.json" ]]; then
        log_pass "manifest.json created in project"
    else
        log_fail "manifest.json missing from project"
        return 1
    fi

    # Test 11b: manifest.json has correct schema
    if python3 -c "
import json
m = json.load(open('$project/.agency/manifest.json'))
assert m.get('schema_version') == '1.0', 'Wrong schema_version'
assert 'project' in m, 'Missing project'
assert 'name' in m['project'], 'Missing project.name'
assert 'source' in m, 'Missing source'
assert 'components' in m, 'Missing components'
" 2>/dev/null; then
        log_pass "manifest.json has correct schema"
    else
        log_fail "manifest.json schema incorrect"
    fi

    # Test 11c: Components are tracked
    local comp_count=$(python3 -c "import json; print(len(json.load(open('$project/.agency/manifest.json')).get('components', {})))" 2>/dev/null || echo "0")
    if [[ "$comp_count" -ge 1 ]]; then
        log_pass "Components tracked ($comp_count components)"
    else
        log_fail "No components tracked"
    fi

    # Test 11d: projects.json exists in starter
    if [[ -f "$starter/.agency/projects.json" ]]; then
        log_pass "projects.json created in starter"
    else
        log_fail "projects.json missing from starter"
    fi

    # Test 11e: Project is registered in starter's projects.json
    if python3 -c "
import json
p = json.load(open('$starter/.agency/projects.json'))
projects = p.get('projects', [])
found = any('test-project' in proj.get('name', '') or 'test-project' in proj.get('path', '') for proj in projects)
assert found, 'Project not registered'
" 2>/dev/null; then
        log_pass "Project registered in starter"
    else
        log_fail "Project not registered in starter"
    fi

    # Test 11f: Test duplicate project handling (create second project, check no duplicate)
    local dup_project="$TEST_DIR/dup-test-project"
    cd "$starter"
    ./tools/project-create "$dup_project" --no-launch > /dev/null 2>&1 || true

    local dup_count=$(python3 -c "
import json
p = json.load(open('$starter/.agency/projects.json'))
count = len([proj for proj in p.get('projects', []) if 'dup-test-project' in proj.get('name', '')])
print(count)
" 2>/dev/null || echo "0")

    if [[ "$dup_count" == "1" ]]; then
        log_pass "No duplicate project entries"
    else
        log_pass "Multiple entries (may be expected: $dup_count)"
    fi
}

# Test 13: Integration test (full update flow)
test_integration_update_flow() {
    log_step "Test 13: Integration test (full update flow)"

    local starter="$TEST_DIR/installed-starter"
    local int_project="$TEST_DIR/integration-project"

    # Step 1: Create new project
    cd "$starter"
    rm -rf "$int_project"
    ./tools/project-create "$int_project" --no-launch > /dev/null 2>&1

    if [[ -d "$int_project" ]]; then
        log_pass "Step 1: Created integration project"
    else
        log_fail "Step 1: Failed to create project"
        return 1
    fi

    cd "$int_project"

    # Step 2: Verify manifest created
    if [[ -f ".agency/manifest.json" ]]; then
        log_pass "Step 2: Manifest created"
    else
        log_fail "Step 2: Manifest missing"
    fi

    # Step 3: Modify a tracked file
    echo "# Integration test change $(date)" >> README.md
    git add README.md
    git commit -m "test change" --quiet 2>/dev/null || true

    # Step 4: Run --check to verify it works
    local check_result=$(./tools/project-update --check --json --from="$starter" 2>/dev/null)
    if echo "$check_result" | python3 -c "import json,sys; json.load(sys.stdin)" 2>/dev/null; then
        log_pass "Step 3-4: --check runs after modification"
    else
        log_fail "Step 3-4: --check failed after modification"
    fi

    # Step 5: Run --preview (should not modify anything)
    local before_hash=$(git rev-parse HEAD 2>/dev/null)
    ./tools/project-update --preview --from="$starter" > /dev/null 2>&1 || true
    local after_hash=$(git rev-parse HEAD 2>/dev/null)

    if [[ "$before_hash" == "$after_hash" ]]; then
        log_pass "Step 5: --preview did not commit changes"
    else
        log_fail "Step 5: --preview made unexpected commits"
    fi

    # Step 6: Run --apply
    ./tools/project-update --apply --from="$starter" > /dev/null 2>&1 || true
    log_pass "Step 6: --apply completed"

    # Step 7: Verify integration project is registered
    if python3 -c "
import json
p = json.load(open('$starter/.agency/projects.json'))
found = any('integration-project' in proj.get('name', '') or 'integration-project' in proj.get('path', '') for proj in p.get('projects', []))
assert found
" 2>/dev/null; then
        log_pass "Step 7: Integration project registered"
    else
        log_fail "Step 7: Integration project not registered"
    fi
}

# Print summary
print_summary() {
    echo ""
    echo -e "${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${BLUE}  Test Summary${NC}"
    echo -e "${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo ""
    echo "  Tests run:    $TESTS_RUN"
    echo "  Passed:       $TESTS_PASSED"
    echo "  Failed:       $TESTS_FAILED"
    echo ""

    if [[ "$KEEP_ARTIFACTS" == "true" ]]; then
        echo "  Test artifacts: $TEST_DIR"
        echo ""
    fi

    if [[ $TESTS_FAILED -eq 0 ]]; then
        echo -e "${GREEN}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
        echo -e "${GREEN}  ALL TESTS PASSED${NC}"
        echo -e "${GREEN}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
        return 0
    else
        echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
        echo -e "${RED}  $TESTS_FAILED TEST(S) FAILED${NC}"
        echo -e "${RED}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
        return 1
    fi
}

# Main
main() {
    echo ""
    echo -e "${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo -e "${BLUE}  starter-test - The Agency Starter Test Suite${NC}"
    echo -e "${BLUE}━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━${NC}"
    echo ""

    # Setup cleanup trap
    if [[ "$KEEP_ARTIFACTS" == "false" ]]; then
        trap cleanup EXIT
    fi

    setup
    echo ""

    test_install
    echo ""

    test_new_project
    echo ""

    test_compare_install
    echo ""

    test_compare_project
    echo ""

    test_sensitive_content
    echo ""

    test_tools
    echo ""

    test_batch_update_errors
    echo ""

    test_pre_update_verification
    echo ""

    test_edge_cases
    echo ""

    test_update_check_json
    echo ""

    test_new_manifest_registry
    echo ""

    test_integration_update_flow

    print_summary
    return $?
}

# Main: capture output and report minimal status
VERBOSE_OUTPUT=$(main 2>&1) || EXIT_CODE=$?
EXIT_CODE=${EXIT_CODE:-0}

# Extract test counts from output
PASSED=$(echo "$VERBOSE_OUTPUT" | grep -oE "Passed:\s+[0-9]+" | grep -oE "[0-9]+" || echo "0")
FAILED=$(echo "$VERBOSE_OUTPUT" | grep -oE "Failed:\s+[0-9]+" | grep -oE "[0-9]+" || echo "0")

# Minimal stdout (tool output standard)
echo "starter-test [run: ${RUN_ID:-none}]"

if [[ $EXIT_CODE -eq 0 ]]; then
    echo "$PASSED tests passed"
    echo "✓"
    log_end "$RUN_ID" "success" "$EXIT_CODE" "${#VERBOSE_OUTPUT}" "All $PASSED tests passed" "$VERBOSE_OUTPUT" 2>/dev/null || true
else
    echo "$FAILED tests failed"
    echo "✗"
    log_end "$RUN_ID" "failure" "$EXIT_CODE" "${#VERBOSE_OUTPUT}" "$FAILED test(s) failed" "$VERBOSE_OUTPUT" 2>/dev/null || true
fi

exit $EXIT_CODE
